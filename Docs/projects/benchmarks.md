# Benchmarks & Tooling

## Purpose
Build public evaluation suites for high-stakes clinical tasks where:
- ambiguity is common
- costs of error are high
- “accuracy” alone is not sufficient

## Principles
- Benchmark tasks must be reproducible, licensed, and ethically sound
- Scoring must reflect usefulness, safety, and uncertainty handling
- Baselines must be included (so results are interpretable)

## What we’ll build
- Task definitions + datasets (public or synthetic)
- Rubrics and scoring scripts
- Calibration reporting templates
- Error analysis playbooks

## Deliverables
- Benchmark spec documents
- Baseline models
- Evaluation dashboards (lightweight)
- Paper/poster-ready results summaries
