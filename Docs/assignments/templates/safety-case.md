# Safety Case (Lightweight)

## System description
## Intended users and decisions supported
## Hazard analysis
List plausible harms if the system fails.

## Failure modes
- Overconfidence / hallucination
- Missingness / ambiguity
- Bias / representativeness
- Workflow misuse
- Adversarial or weird inputs

## Guardrails
- Human-in-the-loop checkpoints
- Escalation triggers
- Refusal / uncertainty behavior
- Logging and monitoring plan

## Safe fallback
What happens when confidence is low or the system detects risk?

## Communication
How limitations are conveyed to the user.
